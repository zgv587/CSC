import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import DataLoader
from transformers import BertModel, BertTokenizer

# Implemented by myself
from config import *
from data_processer import CSCDataset, split_torch_dataset
from models import CombineBertModel, DecoderBaseRNN, DecoderTransformer, Trainer





tokenizer = BertTokenizer.from_pretrained(checkpoint)





train_dataset = CSCDataset([SIGHAN_train_dir_err, SIGHAN_train_dir_corr], tokenizer)
test_dataset = CSCDataset([SIGHAN_train_dir_err14, SIGHAN_train_dir_corr14], tokenizer)


# split data
train_data, dev_data = split_torch_dataset(train_dataset, 0.3)

train_data_loader = DataLoader(train_data, num_workers=4, shuffle=True, batch_size=16)
dev_data_loader = DataLoader(dev_data, num_workers=4, shuffle=True, batch_size=16)
test_data_loader = DataLoader(test_dataset, num_workers=4, shuffle=True, batch_size=32)





# epochs = 35





# The Hyperparameters can be defined in config.py
hidden_size = 1024
num_layers = 2

encoder_model = BertModel.from_pretrained(checkpoint)
decoder_model = DecoderBaseRNN(
    model=nn.LSTM,
    input_size=encoder_model.config.hidden_size,
    hidden_size=hidden_size,
    num_layers=num_layers,
)
model = CombineBertModel(encoder_model=encoder_model, decoder_model=decoder_model)

optimizer = AdamW(model.parameters(), lr=learning_rate)
trainer = Trainer(model=model, tokenizer=tokenizer, optimizer=optimizer)


trainer.train(
    dataloader=train_data_loader, epoch=epochs, test_dataloader=dev_data_loader
)
trainer.test(test_data_loader)


model = torch.load(save_path)


# import time
# from utils import cal_err

# def split_lines(i, length=20):
#     print(f"第{i}句".center(length, '-'))

# def effectiveness_of_beam(model, test_data, beam_width):
#     begin_time = time.time()
#     matrices = ["over_corr", "total_err", "true_corr"]
#     test_char_level = {key: 0 for key in matrices}
#     test_sent_level = {key: 0 for key in matrices}
    
#     for i, data in enumerate(test_data, 1):
#         best_score, best_sequence = -float('inf'), None
#         beam = model.generate_with_beam(beam_width, data)
#         for score, seq in beam:
#             if score > best_score:
#                 best_score = score
#                 best_sequence = seq
#         split_lines(i)
#         input_ids = data["input_ids"]
#         labels = data["labels"]
#         attention_mask = data["attention_mask"]

#         raw_model_output = model(input_ids.resize(1, len(input_ids)).to("cuda"), 
#                   attention_mask.resize(1, len(input_ids)).to("cuda"))

#         length = len(best_sequence)
#         char_level, sent_level = cal_err(
#             input_ids[:length],
#             torch.tensor(best_sequence),
#             labels[:length],
#             length,
#         )
#         test_char_level = {
#             key: test_char_level[key] + v
#             for key, v in char_level.items()
#         }
#         test_sent_level = {
#             key: test_sent_level[key] + v
#             for key, v in sent_level.items()
#         }
            
#         print(f"origin sentence:  {tokenizer.decode(input_ids, skip_special_tokens=True)}")
#         print(f"correct sentence: {tokenizer.decode(labels, skip_special_tokens=True)}")
#         print(f"predict sentence: {tokenizer.decode(best_sequence, skip_special_tokens=True)}")
#         print("r-model sentence:", tokenizer.decode(raw_model_output.argmax(dim=-1).squeeze()[:length], skip_special_tokens=True))
    
#     end_time = time.time()
#     print(f"It cost total {end_time - begin_time} time")

# effectiveness_of_beam(model, test_dataset, 4)


save_path = "weights/BertLstm.pt"
model.save(save_path)





# The Hyperparameters can be defined in config.py
hidden_size = 1024
num_layers = 2

encoder_model = BertModel.from_pretrained(checkpoint)
decoder_model = DecoderBaseRNN(
    model=nn.GRU,
    input_size=encoder_model.config.hidden_size,
    hidden_size=hidden_size,
    num_layers=num_layers,
)
model = CombineBertModel(encoder_model=encoder_model, decoder_model=decoder_model)

train(model, tokenizer, train_data_loader, dev_data_loader)


trainer.train(
    dataloader=train_data_loader, epoch=epochs, test_dataloader=dev_data_loader
)
trainer.test(test_data_loader)


effectiveness_of_beam(model, test_dataset, 4)


save_path = "weights/BertGru.pt"
model.save(save_path)





nhead = 2
num_encoder_layers = 2
num_decoder_layers = 2

encoder_model = BertModel.from_pretrained(checkpoint)
decoder_model = DecoderTransformer(
    input_size=encoder_model.config.hidden_size,
    nhead=nhead,
    num_encoder_layers=num_encoder_layers,
    num_decoder_layers=num_decoder_layers,
)
model = CombineBertModel(encoder_model=encoder_model, decoder_model=decoder_model)

train(model, tokenizer, train_data_loader, dev_data_loader)


trainer.train(
    dataloader=train_data_loader, epoch=epochs, test_dataloader=dev_data_loader
)
trainer.test(test_data_loader)


save_path = "weights/BertTransformer.pt"
model.save(save_path)
