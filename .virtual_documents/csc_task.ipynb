from torch.utils.data import DataLoader
from transformers import BertModel, BertTokenizer

from config import *
from data_processer import CSCDataset


import numpy as np
import torch
import torch.nn.functional as F
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import (
    AdamW,
    BertForSequenceClassification,
    BertTokenizer,
    get_scheduler,
)


def cal_err(raw_sentence, pred_sentence, corr_sentence):
    matrices = ["over_corr", "total_err", "true_corr"]
    char_level = {key: 0 for key in matrices}
    sent_level = {key: 0 for key in matrices}

    f1 = f2 = 0
    for i, c in enumerate(raw_sentence):
        pc, cc = pred_sentence[i], corr_sentence[i]

        if cc != c:
            char_level["total_err"] += 1
            char_level["true_corr"] += pc == cc
            f1 = 1
        elif pc != cc:
            char_level["over_corr"] += 1
            f2 = 1

    if f1:
        sent_level["true_corr"] += all(pred_sentence == corr_sentence)
        sent_level["total_err"] += f1
    sent_level["over_corr"] += f2

    return char_level, sent_level


def test(model, tokenizer, test_data_loader):
    model.eval()
    total_loss = 0
    matrices = ["over_corr", "total_err", "true_corr"]
    test_char_level = {key: 0 for key in matrices}
    test_sent_level = {key: 0 for key in matrices}

    with torch.no_grad():
        for batch in test_data_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device).type(torch.float)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, src_mask=attention_mask)
            logits = outputs.permute(0, 2, 1)

            loss = cross_entropy(logits, labels, ignore_index=tokenizer.pad_token_id)
            total_loss += loss.item()

            t = torch.argmax(outputs, dim=-1)
            nt = t * attention_mask
            pred = tokenizer.batch_decode(nt, skip_special_tokens=True)

            for i in range(len(t)):
                char_level, sent_level = cal_err(input_ids[i], nt[i], labels[i])
                test_char_level = {
                    key: test_char_level[key] + v for key, v in char_level.items()
                }
                test_sent_level = {
                    key: test_sent_level[key] + v for key, v in sent_level.items()
                }
        print(total_loss / len(test_data_loader), test_char_level, test_sent_level)


from torch.nn.functional import cross_entropy
from tqdm import tqdm

from config import *
from models import Seq2SeqModel

# temp_data
nhead = 4  # 多头注意力机制的头数
num_decoder_layers = 2
dim_feedforward = 3072
max_seq_len = 128
dropout = 0.1

if __name__ == "__main__":
    tokenizer = BertTokenizer.from_pretrained(checkpoint)
    bert = BertModel.from_pretrained(checkpoint)

    train_dataset = CSCDataset([SIGHAN_train_dir_err, SIGHAN_train_dir_corr], tokenizer)
    train_data_loader = DataLoader(
        train_dataset, num_workers=0, shuffle=True, batch_size=16
    )

    test_dataset = CSCDataset(
        [SIGHAN_train_dir_err14, SIGHAN_train_dir_corr14], tokenizer
    )
    test_data_loader = DataLoader(
        test_dataset, num_workers=0, shuffle=True, batch_size=32
    )

    model = Seq2SeqModel(
        bert, nhead, num_decoder_layers, dim_feedforward, max_seq_len, dropout
    )

    optimizer = AdamW(model.parameters(), lr=learning_rate)

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model.to(device)

    epochs = 10
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        progress_bar = tqdm(
            enumerate(train_data_loader),
            desc=f"Epoch:{epoch+1}/{epochs}",
            total=len(train_data_loader),
        )

        for i, batch in progress_bar:
            optimizer.zero_grad()

            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device).type(torch.float)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, src_mask=attention_mask)
            logits = outputs.permute(0, 2, 1)  # (batch_size, vocab_size, seq_len)

            # 反向传播在这，故labels不需要传入模型
            loss = cross_entropy(logits, labels, ignore_index=tokenizer.pad_token_id)
            total_loss += loss.item()
            loss.backward()
            optimizer.step()

            progress_bar.set_postfix({"loss": "{:.3f}".format(loss.item())})

        t = torch.argmax(outputs, dim=-1)
        nt = t * attention_mask
        pred = tokenizer.batch_decode(nt, skip_special_tokens=True)
        # print(pred)
        # print(f"origin{tokenizer.batch_decode(labels, skip_special_tokens=True)}")

        for i, v in enumerate(nt):
            r, l = input_ids[i], labels[i]
            print(tokenizer.decode(r, skip_special_tokens=True))
            print(tokenizer.decode(v, skip_special_tokens=True))
            print(tokenizer.decode(l, skip_special_tokens=True))
            print(cal_err(r, v, l))

        print(f"Epoch {epoch+1} Loss: {total_loss / len(train_data_loader)}")
        test(model, tokenizer, test_data_loader)


for epoch in range(epochs):
    model.train()
    total_loss = 0
    progress_bar = tqdm(
        enumerate(train_data_loader),
        desc=f"Epoch:{epoch+1}/{epochs}",
        total=len(train_data_loader),
    )

    for i, batch in progress_bar:
        optimizer.zero_grad()

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device).type(torch.float)
        labels = batch["labels"].to(device)

        outputs = model(input_ids, src_mask=attention_mask)
        logits = outputs.permute(0, 2, 1)  # (batch_size, vocab_size, seq_len)

        # 反向传播在这，故labels不需要传入模型
        loss = cross_entropy(logits, labels, ignore_index=tokenizer.pad_token_id)
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

        progress_bar.set_postfix({"loss": "{:.3f}".format(loss.item())})

    t = torch.argmax(outputs, dim=-1)
    nt = t * attention_mask
    pred = tokenizer.batch_decode(nt, skip_special_tokens=True)
    # print(pred)
    # print(f"origin{tokenizer.batch_decode(labels, skip_special_tokens=True)}")

    for i, v in enumerate(nt):
        r, l = input_ids[i], labels[i]
        print(tokenizer.decode(r, skip_special_tokens=True))
        print(tokenizer.decode(v, skip_special_tokens=True))
        print(tokenizer.decode(l, skip_special_tokens=True))
        print(cal_err(r, v, l))

    print(f"Epoch {epoch+1} Loss: {total_loss / len(train_data_loader)}")
    test(model, tokenizer, test_data_loader)





class Trainer:
    def __init__(self, model):
        self.model = model
        self.matrices = ["over_corr", "total_err", "true_corr"]

    def train(self, dataloader, epoch):
        self.iteration(dataloader, epoch)

    def test(self, dataloader):
        self.iteration(dataloader, train=False)

    def iteration(self, dataloader, epochs=1, train=True):
        mode = "train" if train else "dev"
        model.train() if train else model.eval()

        for epoch in range(epochs):
            # matrices
            total_loss = 0
            char_level = {key: 0 for key in self.matrices}
            sent_level = {key: 0 for key in self.matrices}

            progress_bar = tqdm(
                dataloader,
                desc=f"{mode} Epoch:{epoch+1}/{epochs}",
                total=len(dataloader),
            )
            for batch in progress_bar:
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)

                outputs = model(input_ids, src_mask=attention_mask, tgt=labels)
                logits = outputs.permute(0, 2, 1)  # (batch_size, vocab_size, seq_len)

                loss = cross_entropy(
                    logits, labels, ignore_index=tokenizer.pad_token_id
                )

                total_loss += loss.item()
                progress_bar.set_postfix({"loss": "{:.3f}".format(loss.item())})

    def cal_err(raw_sentence, pred_sentence, corr_sentence):
        char_level = {key: 0 for key in self.matrices}
        sent_level = {key: 0 for key in self.matrices}

        for i, c in enumerate(raw_sentence):
            pc, cc = pred_sentence[i], corr_sentence[i]
            f1 = f2 = False

            if cc != c:
                char_level["total_err"] += 1
                char_level["true_corr"] += pc == cc
                f1 = True
            elif pc != cc:
                char_level["over_corr"] += 1
                f2 = True

        # true_corr 未计算
        sent_level["total_err"] += f1
        sent_level["over_corr"] += f2

        return char_level, sent_level
