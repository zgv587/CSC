import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
from transformers import BertModel, BertTokenizer

# Implemented by myself
from config import *
from data_processer import CSCDataset, split_torch_dataset
from models import CombineBertModel, DecoderBaseRNN, DecoderTransformer


from utils import cal_err

# def cal_err(raw_sentence, pred_sentence, corr_sentence, limit_length=300):
#     matrices = ["over_corr", "total_err", "true_corr"]
#     char_level = {key: 0 for key in matrices}
#     sent_level = {key: 0 for key in matrices}

#     f1 = f2 = 0
#     for i, c in enumerate(raw_sentence):
#         if i >= limit_length:
#             break

#         pc, cc = pred_sentence[i], corr_sentence[i]

#         if cc != c:
#             char_level["total_err"] += 1
#             char_level["true_corr"] += pc == cc
#             f1 = 1
#         elif pc != cc:
#             char_level["over_corr"] += 1
#             f2 = 1

#     if f1:
#         sent_level["true_corr"] += all(pred_sentence == corr_sentence)
#         sent_level["total_err"] += f1
#     sent_level["over_corr"] += f2

#     return char_level, sent_level


tokenizer = BertTokenizer.from_pretrained(checkpoint)


# def split_torch_dataset(dataset, test_size=0.2, random_seed=None):
#     """
#     将PyTorch Dataset对象划分为训练集和测试集。

#     参数:
#         dataset (Dataset): PyTorch的Dataset对象。
#         test_size (float or int): 如果是浮点数，表示测试集相对于整个数据集的比例；如果是整数，表示测试集的样本数量。默认值为0.2。
#         random_seed (int or None): 随机种子，用于控制随机性以确保结果可复现。默认值为None。

#     返回:
#         tuple: 一个包含两个Dataset对象的元组 (train_dataset, test_dataset)。
#     """
#     # 计算训练集和测试集的大小
#     total_length = len(dataset)
#     if isinstance(test_size, float):
#         test_length = int(total_length * test_size)
#     else:
#         test_length = test_size
#     train_length = total_length - test_length

#     # 设置随机种子
#     generator = (
#         torch.Generator().manual_seed(random_seed) if random_seed is not None else None
#     )

#     # 分割数据集
#     train_dataset, test_dataset = random_split(
#         dataset, [train_length, test_length], generator=generator
#     )

#     return train_dataset, test_dataset


train_dataset = CSCDataset([SIGHAN_train_dir_err, SIGHAN_train_dir_corr], tokenizer)

test_dataset = CSCDataset([SIGHAN_train_dir_err14, SIGHAN_train_dir_corr14], tokenizer)


train_data, dev_data = split_torch_dataset(train_dataset, 0.3)

train_data_loader = DataLoader(train_data, num_workers=4, shuffle=True, batch_size=16)

dev_data_loader = DataLoader(dev_data, num_workers=4, shuffle=True, batch_size=16)

test_data_loader = DataLoader(test_dataset, num_workers=4, shuffle=True, batch_size=32)


def train(model, tokenizer, train_data_loader, test_data_loader=None):
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model.to(device)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        progress_bar = tqdm(
            enumerate(train_data_loader),
            desc=f"Epoch:{epoch+1}/{epochs}",
            total=len(train_data_loader),
        )

        for i, batch in progress_bar:
            optimizer.zero_grad()

            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device).type(torch.float)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, src_mask=attention_mask)
            logits = outputs.permute(0, 2, 1)  # (batch_size, vocab_size, seq_len)

            # 反向传播在这，故labels不需要传入模型
            loss = F.cross_entropy(logits, labels, ignore_index=tokenizer.pad_token_id)
            total_loss += loss.item()
            loss.backward()
            optimizer.step()

            progress_bar.set_postfix({"loss": "{:.3f}".format(loss.item())})

        # if epoch % 5 == 0:
        with torch.no_grad():
            t = torch.argmax(outputs, dim=-1)
            nt = t * attention_mask
            pred = tokenizer.batch_decode(nt, skip_special_tokens=True)
        # print(pred)
        # print(f"origin{tokenizer.batch_decode(labels, skip_special_tokens=True)}")

        for i, v in enumerate(nt):
            r, l = input_ids[i], labels[i]
            limit_length = sum(attention_mask[i].to("cpu"))
            print(tokenizer.decode(r, skip_special_tokens=True))
            print(tokenizer.decode(v, skip_special_tokens=True))
            print(tokenizer.decode(l, skip_special_tokens=True))
            print(cal_err(r, v, l, limit_length))
            # print(cal_err(r, v, l))

        print(f"Epoch {epoch+1} Loss: {total_loss / len(train_data_loader)}")

        # dev
        if test_data_loader:
            test(model, tokenizer, test_data_loader)

        # 释放不必要的内存
        # del input_ids, attention_mask, labels, outputs, loss
        # torch.cuda.empty_cache()
        # gc.collect()


def test(model, tokenizer, test_data_loader):
    model.eval()
    total_loss = 0
    matrices = ["over_corr", "total_err", "true_corr"]
    test_char_level = {key: 0 for key in matrices}
    test_sent_level = {key: 0 for key in matrices}
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    with torch.no_grad():
        for batch in test_data_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device).type(torch.float)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, src_mask=attention_mask)
            logits = outputs.permute(0, 2, 1)

            loss = F.cross_entropy(logits, labels, ignore_index=tokenizer.pad_token_id)
            total_loss += loss.item()

            t = torch.argmax(outputs, dim=-1)
            nt = t * attention_mask
            pred = tokenizer.batch_decode(nt, skip_special_tokens=True)

            for i in range(len(t)):
                char_level, sent_level = cal_err(
                    input_ids[i], nt[i], labels[i], sum(attention_mask[i].to("cpu"))
                )
                test_char_level = {
                    key: test_char_level[key] + v for key, v in char_level.items()
                }
                test_sent_level = {
                    key: test_sent_level[key] + v for key, v in sent_level.items()
                }
        print(total_loss / len(test_data_loader), test_char_level, test_sent_level)





epochs = 35

# tokenizer = BertTokenizer.from_pretrained(checkpoint)
encoder_model = BertModel.from_pretrained(checkpoint)

# The Hyperparameters can be defined in config.py
hidden_size = 1024
num_layers = 2

decoder_model = DecoderBaseRNN(
    model=nn.LSTM,
    input_size=encoder_model.config.hidden_size,
    hidden_size=hidden_size,
    num_layers=num_layers,
)

model = CombineBertModel(encoder_model=encoder_model, decoder_model=decoder_model)

train(model, tokenizer, train_data_loader, dev_data_loader)


test(model, tokenizer, test_data_loader)





encoder_model = BertModel.from_pretrained(checkpoint)

# The Hyperparameters can be defined in config.py
hidden_size = 1024
num_layers = 2

decoder_model = DecoderBaseRNN(
    model=nn.GRU,
    input_size=encoder_model.config.hidden_size,
    hidden_size=hidden_size,
    num_layers=num_layers,
)

model = CombineBertModel(encoder_model=encoder_model, decoder_model=decoder_model)

train(model, tokenizer, train_data_loader, dev_data_loader)


test(model, tokenizer, test_data_loader)





encoder_model = BertModel.from_pretrained(checkpoint)

nhead = 2
num_encoder_layers = 2
num_decoder_layers = 2

decoder_model = DecoderTransformer(
    input_size=encoder_model.config.hidden_size,
    nhead=nhead,
    num_encoder_layers=num_encoder_layers,
    num_decoder_layers=num_decoder_layers,
)

model = CombineBertModel(encoder_model=encoder_model, decoder_model=decoder_model)

train(model, tokenizer, train_data_loader, dev_data_loader)


test(model, tokenizer, test_data_loader)


123



