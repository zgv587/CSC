{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4707a58-178e-4371-acc9-b2a590cd0424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:15:30.685522Z",
     "iopub.status.busy": "2024-12-03T12:15:30.685522Z",
     "iopub.status.idle": "2024-12-03T12:15:34.234237Z",
     "shell.execute_reply": "2024-12-03T12:15:34.234237Z",
     "shell.execute_reply.started": "2024-12-03T12:15:30.685522Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb2acbe-8f4a-429a-b0a6-449d0d7b7b0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:15:34.235247Z",
     "iopub.status.busy": "2024-12-03T12:15:34.235247Z",
     "iopub.status.idle": "2024-12-03T12:15:35.269829Z",
     "shell.execute_reply": "2024-12-03T12:15:35.269829Z",
     "shell.execute_reply.started": "2024-12-03T12:15:34.235247Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-chinese\"\n",
    "\n",
    "bert_model = BertModel.from_pretrained(checkpoint)\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353f5c0c-80c6-4cf1-8f57-c1f8baa25ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:15:35.269829Z",
     "iopub.status.busy": "2024-12-03T12:15:35.269829Z",
     "iopub.status.idle": "2024-12-03T12:15:35.274290Z",
     "shell.execute_reply": "2024-12-03T12:15:35.274290Z",
     "shell.execute_reply.started": "2024-12-03T12:15:35.269829Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"我是大帅哥\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "194025f6-db73-43a1-a744-77f7f638d371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:54:50.728026Z",
     "iopub.status.busy": "2024-12-03T12:54:50.728026Z",
     "iopub.status.idle": "2024-12-03T12:54:50.734734Z",
     "shell.execute_reply": "2024-12-03T12:54:50.734734Z",
     "shell.execute_reply.started": "2024-12-03T12:54:50.728026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769, 3221, 1920, 2358, 1520,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(sentence, return_tensors=\"pt\")\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23ec8c26-a000-46f5-83de-97b392dd7107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:57:23.894136Z",
     "iopub.status.busy": "2024-12-03T12:57:23.893121Z",
     "iopub.status.idle": "2024-12-03T12:57:23.930527Z",
     "shell.execute_reply": "2024-12-03T12:57:23.930527Z",
     "shell.execute_reply.started": "2024-12-03T12:57:23.894136Z"
    }
   },
   "outputs": [],
   "source": [
    "res = bert_model(t[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b1e562-e297-445a-8fa9-30857b230e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:09:14.634394Z",
     "iopub.status.busy": "2024-12-03T15:09:14.632889Z",
     "iopub.status.idle": "2024-12-03T15:09:34.287589Z",
     "shell.execute_reply": "2024-12-03T15:09:34.287081Z",
     "shell.execute_reply.started": "2024-12-03T15:09:14.634394Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "from config import *\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CSCDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: Union[str, List[str]],  # path 数据集路径\n",
    "    ):\n",
    "        # assert len(data) == len(label)\n",
    "        self.path = path\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        self.data_processor()\n",
    "\n",
    "    def data_processor(self):\n",
    "        if isinstance(self.path, list):\n",
    "            self.handle_sighan()\n",
    "        elif isinstance(self.path, str):\n",
    "            pass\n",
    "\n",
    "    def handle_sighan(self):\n",
    "        xpath, ypath = self.path\n",
    "        num_lines = 0\n",
    "        with open(xpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm.tqdm(f, desc=\"preprocessing sighan dataset\"):\n",
    "                line = line.strip()\n",
    "                num_lines += 1\n",
    "                self.x.append(line)\n",
    "\n",
    "        with open(ypath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm.tqdm(\n",
    "                f, desc=\"preprocessing sighan dataset\", total=num_lines\n",
    "            ):\n",
    "                line = line.strip()\n",
    "                self.y.append(line)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9afe2336-11c9-4ed9-9256-648466e04416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:09:34.298119Z",
     "iopub.status.busy": "2024-12-03T15:09:34.297113Z",
     "iopub.status.idle": "2024-12-03T15:09:34.354010Z",
     "shell.execute_reply": "2024-12-03T15:09:34.354010Z",
     "shell.execute_reply.started": "2024-12-03T15:09:34.298119Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing sighan dataset: 700it [00:00, 21563.44it/s]\n",
      "preprocessing sighan dataset: 100%|██████████████████████████████████████████████| 700/700 [00:00<00:00, 955733.33it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CSCDataset(path=[SIGHAN_train_dir_corr, SIGHAN_train_dir_err])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e2e084-6127-4ff6-ae3e-7915decf4640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:09:34.355020Z",
     "iopub.status.busy": "2024-12-03T15:09:34.355020Z",
     "iopub.status.idle": "2024-12-03T15:09:34.357835Z",
     "shell.execute_reply": "2024-12-03T15:09:34.357835Z",
     "shell.execute_reply.started": "2024-12-03T15:09:34.355020Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=0, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6baa243-fcdb-410a-ae97-0b71673ad1ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:09:34.358840Z",
     "iopub.status.busy": "2024-12-03T15:09:34.358840Z",
     "iopub.status.idle": "2024-12-03T15:09:34.465939Z",
     "shell.execute_reply": "2024-12-03T15:09:34.464847Z",
     "shell.execute_reply.started": "2024-12-03T15:09:34.358840Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbert_model\u001b[49m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bert_model' is not defined"
     ]
    }
   ],
   "source": [
    "bert_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6376f88-adcd-4459-9e5a-20b81f4d6ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd39f012-3688-49a1-ab32-b0384cfb795b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:10:44.629264Z",
     "iopub.status.busy": "2024-12-03T15:10:44.629264Z",
     "iopub.status.idle": "2024-12-03T15:11:16.341470Z",
     "shell.execute_reply": "2024-12-03T15:11:16.341470Z",
     "shell.execute_reply.started": "2024-12-03T15:10:44.629264Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "284e07a0-b638-442a-baab-17f0ffb13115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:11:16.342477Z",
     "iopub.status.busy": "2024-12-03T15:11:16.342477Z",
     "iopub.status.idle": "2024-12-03T15:11:20.232134Z",
     "shell.execute_reply": "2024-12-03T15:11:20.232134Z",
     "shell.execute_reply.started": "2024-12-03T15:11:16.342477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-chinese\", num_labels=tokenizer.vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82738bc-9ca0-4faa-9848-5072ac18ad74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:11:20.233151Z",
     "iopub.status.busy": "2024-12-03T15:11:20.233151Z",
     "iopub.status.idle": "2024-12-03T15:11:20.242466Z",
     "shell.execute_reply": "2024-12-03T15:11:20.242466Z",
     "shell.execute_reply.started": "2024-12-03T15:11:20.233151Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing sighan dataset: 700it [00:00, 1358636.19it/s]\n",
      "preprocessing sighan dataset: 100%|██████████████████████████████████████████████| 700/700 [00:00<00:00, 672934.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = CSCDataset(path=[SIGHAN_train_dir_err, SIGHAN_train_dir_corr])\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=4, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16002253-7d92-40cb-918a-917cbd68bfb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:11:20.244476Z",
     "iopub.status.busy": "2024-12-03T15:11:20.244476Z",
     "iopub.status.idle": "2024-12-03T15:11:25.039977Z",
     "shell.execute_reply": "2024-12-03T15:11:25.039030Z",
     "shell.execute_reply.started": "2024-12-03T15:11:20.244476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=21128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2001175b-9953-465c-a1f4-d0ad6a1346d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:11:25.041962Z",
     "iopub.status.busy": "2024-12-03T15:11:25.041962Z",
     "iopub.status.idle": "2024-12-03T15:11:25.149825Z",
     "shell.execute_reply": "2024-12-03T15:11:25.149825Z",
     "shell.execute_reply.started": "2024-12-03T15:11:25.041962Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9457ffd3-5f3e-4d36-8b33-0bf00128eb46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:11:25.150833Z",
     "iopub.status.busy": "2024-12-03T15:11:25.150833Z",
     "iopub.status.idle": "2024-12-03T15:11:25.155959Z",
     "shell.execute_reply": "2024-12-03T15:11:25.154947Z",
     "shell.execute_reply.started": "2024-12-03T15:11:25.150833Z"
    }
   },
   "outputs": [],
   "source": [
    "# 学习率调度器\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_data_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee9e35-78ee-441e-a40e-d573364252ae",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T15:09:34.472509Z",
     "iopub.status.idle": "2024-12-03T15:09:34.473506Z",
     "shell.execute_reply": "2024-12-03T15:09:34.473506Z",
     "shell.execute_reply.started": "2024-12-03T15:09:34.473506Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in train_data_loader:\n",
    "        input_ids, token_type_ids, attention_mask = tokenizer(\n",
    "            x,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "        ).values()\n",
    "        labels = tokenizer(\n",
    "            y,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "        )[\"input_ids\"]\n",
    "        outputs = model(\n",
    "            input_ids=input_ids.to(device).squeeze(),\n",
    "            attention_mask=attention_mask.to(device).squeeze(),\n",
    "            labels=labels.to(device).squeeze(),\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # 验证步骤可以在这里添加\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea7c97-58c3-4c02-8517-829bf1f93ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dcf2ef-e987-42b7-be84-4c67609caa54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:11:59.735252Z",
     "iopub.status.busy": "2024-12-03T15:11:59.734252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0440bb23e9fa4a94bb2cc376a923115e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6131e4c8f9324c8f8d7b36f447ec1ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, BertForMaskedLM, BertTokenizer, get_scheduler\n",
    "\n",
    "\n",
    "# 创建模拟的CSC数据集\n",
    "# def create_mock_csc_dataset():\n",
    "#     return {\n",
    "#         \"train\": [\n",
    "#             {\"original_text\": \"这是一个测试句子\", \"corrected_text\": \"这是一个测试句子\"},\n",
    "#             {\"original_text\": \"这是个测试句子\", \"corrected_text\": \"这是一个测试句子\"},\n",
    "#             {\"original_text\": \"这是一只猫\", \"corrected_text\": \"这是一只猫\"},\n",
    "#             {\"original_text\": \"我爱北京天安们\", \"corrected_text\": \"我爱北京天安门\"},\n",
    "#         ],\n",
    "#         \"validation\": [\n",
    "#             {\n",
    "#                 \"original_text\": \"这是一个简单的例子\",\n",
    "#                 \"corrected_text\": \"这是一个简单的例子\",\n",
    "#             },\n",
    "#             {\"original_text\": \"他去了北景公园\", \"corrected_text\": \"他去了北海公园\"},\n",
    "#         ],\n",
    "#     }\n",
    "def create_mock_csc_dataset():\n",
    "    ret = {\"train\": [], \"validation\": []}\n",
    "    for x, y in train_data_loader:\n",
    "        for i, v in enumerate(x):\n",
    "            ret[\"train\"].append({\"original_text\": v, \"corrected_text\": y[i]})\n",
    "            ret[\"validation\"].append({\"original_text\": v, \"corrected_text\": y[i]})\n",
    "    return ret\n",
    "\n",
    "\n",
    "dataset = create_mock_csc_dataset()\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "# 定义tokenize函数\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"original_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"corrected_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"]\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": labels.squeeze(),\n",
    "    }\n",
    "\n",
    "\n",
    "# 将数据集转换为Hugging Face的Dataset对象\n",
    "train_dataset = Dataset.from_list(dataset[\"train\"])\n",
    "val_dataset = Dataset.from_list(dataset[\"validation\"])\n",
    "\n",
    "# 对数据集进行tokenize处理\n",
    "train_tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "val_tokenized_datasets = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 设置格式为pytorch tensor\n",
    "train_tokenized_datasets.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "val_tokenized_datasets.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "# 创建DataLoader\n",
    "train_dataloader = DataLoader(train_tokenized_datasets, shuffle=True, batch_size=4)\n",
    "eval_dataloader = DataLoader(val_tokenized_datasets, batch_size=4)\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 检查设备类型\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 优化器设置\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 学习率调度器\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# 训练循环\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # 验证步骤可以在这里添加\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# 示例预测\n",
    "def predict(text):\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_token_id = logits.argmax(dim=-1)[0]\n",
    "    corrected_text = tokenizer.decode(predicted_token_id, skip_special_tokens=True)\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "# 测试预测\n",
    "test_texts = [\"我爱北京天安们\", \"这是一只猫\"]\n",
    "for text in test_texts:\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Corrected: {predict(text)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888153a-ed10-415c-be4a-ebdb4e150943",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"我愛北京天们\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380d033-fbd0-4b2b-9ed8-18e3fa3414b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9b127-d502-47f5-8805-e40f553717f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df0834-896b-4aa4-a072-ca3499ef282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_pos(x, y):\n",
    "    pos = []\n",
    "    for i, v in enumerate(x):\n",
    "        if v != y[i]:\n",
    "            print(v, y[i])\n",
    "            pos.append(i)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06522a23-d36f-46e0-bd09-a82df7ca68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x, y, pos):\n",
    "    err, correct, tot = 0, 0, 0\n",
    "    er_pos = \"\"\n",
    "    for i, v in enumerate(x):\n",
    "        if i in pos:\n",
    "            tot += 1\n",
    "            if v == y[i]:\n",
    "                correct += 1\n",
    "        elif v != y[i]:\n",
    "            err += 1\n",
    "            er_pos += y[i]\n",
    "    return err, correct, tot, er_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7250e33-35d2-4571-9c6f-64c054b5f33e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T15:11:25.171058Z",
     "iopub.status.busy": "2024-12-03T15:11:25.170059Z",
     "iopub.status.idle": "2024-12-03T15:11:25.247295Z",
     "shell.execute_reply": "2024-12-03T15:11:25.246045Z",
     "shell.execute_reply.started": "2024-12-03T15:11:25.171058Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'te' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mte\u001b[49m, tc, tt\n",
      "\u001b[1;31mNameError\u001b[0m: name 'te' is not defined"
     ]
    }
   ],
   "source": [
    "te, tc, tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5285fc6-c4b3-44aa-ba1f-f21d4fc8f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "te, tc, tt = 0, 0, 0\n",
    "\n",
    "for x, y in train_data_loader:\n",
    "    for i, v in enumerate(x):\n",
    "        cy = y[i]\n",
    "        cur_pos = get_error_pos(v, cy)\n",
    "        px = predict(v)\n",
    "        npx = \"\".join(px.split())\n",
    "        if len(npx) == len(cy):\n",
    "            e, c, t, er_pos = score(npx, cy, cur_pos)\n",
    "            te += e\n",
    "            tc += c\n",
    "            tt += t\n",
    "            if e != 0:\n",
    "                print(\"ERROR\", er_pos)\n",
    "            if c != t:\n",
    "                print(\"not true\")\n",
    "        else:\n",
    "            print(\"notice\")\n",
    "\n",
    "        print(i, v, npx, cy, cur_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc5056-2714-4b97-80dd-90ea8b35da06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d0bf01-0235-4e71-b81f-cc2de54aa917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc_env",
   "language": "python",
   "name": "csc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
