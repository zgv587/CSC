{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4707a58-178e-4371-acc9-b2a590cd0424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:15:30.685522Z",
     "iopub.status.busy": "2024-12-03T12:15:30.685522Z",
     "iopub.status.idle": "2024-12-03T12:15:34.234237Z",
     "shell.execute_reply": "2024-12-03T12:15:34.234237Z",
     "shell.execute_reply.started": "2024-12-03T12:15:30.685522Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb2acbe-8f4a-429a-b0a6-449d0d7b7b0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:15:34.235247Z",
     "iopub.status.busy": "2024-12-03T12:15:34.235247Z",
     "iopub.status.idle": "2024-12-03T12:15:35.269829Z",
     "shell.execute_reply": "2024-12-03T12:15:35.269829Z",
     "shell.execute_reply.started": "2024-12-03T12:15:34.235247Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-chinese\"\n",
    "\n",
    "bert_model = BertModel.from_pretrained(checkpoint)\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353f5c0c-80c6-4cf1-8f57-c1f8baa25ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:15:35.269829Z",
     "iopub.status.busy": "2024-12-03T12:15:35.269829Z",
     "iopub.status.idle": "2024-12-03T12:15:35.274290Z",
     "shell.execute_reply": "2024-12-03T12:15:35.274290Z",
     "shell.execute_reply.started": "2024-12-03T12:15:35.269829Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"我是大帅哥\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "194025f6-db73-43a1-a744-77f7f638d371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:54:50.728026Z",
     "iopub.status.busy": "2024-12-03T12:54:50.728026Z",
     "iopub.status.idle": "2024-12-03T12:54:50.734734Z",
     "shell.execute_reply": "2024-12-03T12:54:50.734734Z",
     "shell.execute_reply.started": "2024-12-03T12:54:50.728026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769, 3221, 1920, 2358, 1520,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(sentence, return_tensors=\"pt\")\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23ec8c26-a000-46f5-83de-97b392dd7107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:57:23.894136Z",
     "iopub.status.busy": "2024-12-03T12:57:23.893121Z",
     "iopub.status.idle": "2024-12-03T12:57:23.930527Z",
     "shell.execute_reply": "2024-12-03T12:57:23.930527Z",
     "shell.execute_reply.started": "2024-12-03T12:57:23.894136Z"
    }
   },
   "outputs": [],
   "source": [
    "res = bert_model(t[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b1e562-e297-445a-8fa9-30857b230e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T13:37:09.100579Z",
     "iopub.status.busy": "2024-12-03T13:37:09.100579Z",
     "iopub.status.idle": "2024-12-03T13:37:09.204603Z",
     "shell.execute_reply": "2024-12-03T13:37:09.204603Z",
     "shell.execute_reply.started": "2024-12-03T13:37:09.100579Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "from config import *\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CSCDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: Union[str, List[str]],  # path 数据集路径\n",
    "    ):\n",
    "        # assert len(data) == len(label)\n",
    "        self.path = path\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        self.data_processor()\n",
    "\n",
    "    def data_processor(self):\n",
    "        if isinstance(self.path, list):\n",
    "            self.handle_sighan()\n",
    "        elif isinstance(self.path, str):\n",
    "            pass\n",
    "\n",
    "    def handle_sighan(self):\n",
    "        xpath, ypath = self.path\n",
    "        num_lines = 0\n",
    "        with open(xpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm.tqdm(f, desc=\"preprocessing sighan dataset\"):\n",
    "                line = line.strip()\n",
    "                num_lines += 1\n",
    "                self.x.append(line)\n",
    "\n",
    "        with open(ypath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in tqdm.tqdm(\n",
    "                f, desc=\"preprocessing sighan dataset\", total=num_lines\n",
    "            ):\n",
    "                line = line.strip()\n",
    "                self.y.append(line)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afe2336-11c9-4ed9-9256-648466e04416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:15:35.367252Z",
     "iopub.status.busy": "2024-12-03T12:15:35.367252Z",
     "iopub.status.idle": "2024-12-03T12:15:35.377697Z",
     "shell.execute_reply": "2024-12-03T12:15:35.377172Z",
     "shell.execute_reply.started": "2024-12-03T12:15:35.367252Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing sighan dataset: 700it [00:00, ?it/s]\n",
      "preprocessing sighan dataset: 100%|██████████████████████████████████████████████████████████| 700/700 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CSCDataset(path=[SIGHAN_train_dir_corr, SIGHAN_train_dir_err])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94e2e084-6127-4ff6-ae3e-7915decf4640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T13:37:35.767550Z",
     "iopub.status.busy": "2024-12-03T13:37:35.766529Z",
     "iopub.status.idle": "2024-12-03T13:37:35.770271Z",
     "shell.execute_reply": "2024-12-03T13:37:35.770271Z",
     "shell.execute_reply.started": "2024-12-03T13:37:35.767550Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=0, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6baa243-fcdb-410a-ae97-0b71673ad1ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T12:58:06.774056Z",
     "iopub.status.busy": "2024-12-03T12:58:06.774056Z",
     "iopub.status.idle": "2024-12-03T12:58:06.781424Z",
     "shell.execute_reply": "2024-12-03T12:58:06.781424Z",
     "shell.execute_reply.started": "2024-12-03T12:58:06.774056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6376f88-adcd-4459-9e5a-20b81f4d6ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd39f012-3688-49a1-ab32-b0384cfb795b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T13:37:19.831033Z",
     "iopub.status.busy": "2024-12-03T13:37:19.831033Z",
     "iopub.status.idle": "2024-12-03T13:37:19.837579Z",
     "shell.execute_reply": "2024-12-03T13:37:19.837062Z",
     "shell.execute_reply.started": "2024-12-03T13:37:19.831033Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "284e07a0-b638-442a-baab-17f0ffb13115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T13:37:20.662242Z",
     "iopub.status.busy": "2024-12-03T13:37:20.662242Z",
     "iopub.status.idle": "2024-12-03T13:37:21.791236Z",
     "shell.execute_reply": "2024-12-03T13:37:21.791236Z",
     "shell.execute_reply.started": "2024-12-03T13:37:20.662242Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-chinese\", num_labels=tokenizer.vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82738bc-9ca0-4faa-9848-5072ac18ad74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T13:37:21.793247Z",
     "iopub.status.busy": "2024-12-03T13:37:21.792247Z",
     "iopub.status.idle": "2024-12-03T13:37:21.929203Z",
     "shell.execute_reply": "2024-12-03T13:37:21.929203Z",
     "shell.execute_reply.started": "2024-12-03T13:37:21.793247Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing sighan dataset: 700it [00:00, 6465.48it/s]\n",
      "preprocessing sighan dataset: 100%|██████████████████████████████████████████████████████████| 700/700 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = CSCDataset(path=[SIGHAN_train_dir_corr, SIGHAN_train_dir_err])\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=4, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16002253-7d92-40cb-918a-917cbd68bfb7",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-03T13:37:23.219090Z",
     "iopub.status.busy": "2024-12-03T13:37:23.219090Z",
     "iopub.status.idle": "2024-12-03T13:37:25.425120Z",
     "shell.execute_reply": "2024-12-03T13:37:25.425120Z",
     "shell.execute_reply.started": "2024-12-03T13:37:23.219090Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=21128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2001175b-9953-465c-a1f4-d0ad6a1346d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T13:37:25.425120Z",
     "iopub.status.busy": "2024-12-03T13:37:25.425120Z",
     "iopub.status.idle": "2024-12-03T13:37:25.431965Z",
     "shell.execute_reply": "2024-12-03T13:37:25.431965Z",
     "shell.execute_reply.started": "2024-12-03T13:37:25.425120Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9457ffd3-5f3e-4d36-8b33-0bf00128eb46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T13:37:56.665181Z",
     "iopub.status.busy": "2024-12-03T13:37:56.665181Z",
     "iopub.status.idle": "2024-12-03T13:37:56.668445Z",
     "shell.execute_reply": "2024-12-03T13:37:56.668445Z",
     "shell.execute_reply.started": "2024-12-03T13:37:56.665181Z"
    }
   },
   "outputs": [],
   "source": [
    "# 学习率调度器\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_data_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6ee9e35-78ee-441e-a40e-d573364252ae",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-03T13:40:08.678562Z",
     "iopub.status.busy": "2024-12-03T13:40:08.678562Z",
     "iopub.status.idle": "2024-12-03T13:40:14.409515Z",
     "shell.execute_reply": "2024-12-03T13:40:14.408247Z",
     "shell.execute_reply.started": "2024-12-03T13:40:08.678562Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1762, 2823,  ...,    0,    0,    0],\n",
      "        [ 101, 4534, 6868,  ...,    0,    0,    0],\n",
      "        [ 101, 2769, 4638,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3292, 3315,  ...,    0,    0,    0],\n",
      "        [ 101, 3300, 1003,  ...,    0,    0,    0],\n",
      "        [ 101, 6847, 1862,  ...,    0,    0,    0]]) 64\n",
      "64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (64) to match target batch_size (8192).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels, \u001b[38;5;28mlen\u001b[39m(labels))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels))\n\u001b[1;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1703\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1702\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1703\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1705\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (64) to match target batch_size (8192)."
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in train_data_loader:\n",
    "        input_ids, token_type_ids, attention_mask = tokenizer(\n",
    "            x,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "        ).values()\n",
    "        labels = tokenizer(\n",
    "            y,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "        )[\"input_ids\"]\n",
    "        print(labels, len(labels))\n",
    "        print(len(labels))\n",
    "        outputs = model(\n",
    "            input_ids=input_ids.to(device).squeeze(),\n",
    "            attention_mask=attention_mask.to(device).squeeze(),\n",
    "            labels=labels.to(device).squeeze(),\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # 验证步骤可以在这里添加\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea7c97-58c3-4c02-8517-829bf1f93ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4dcf2ef-e987-42b7-be84-4c67609caa54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T13:55:37.753431Z",
     "iopub.status.busy": "2024-12-03T13:55:37.753431Z",
     "iopub.status.idle": "2024-12-03T13:56:47.985573Z",
     "shell.execute_reply": "2024-12-03T13:56:47.985573Z",
     "shell.execute_reply.started": "2024-12-03T13:55:37.753431Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m             ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrected_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: y[i]})\n\u001b[0;32m     29\u001b[0m             ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrected_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: y[i]})\n\u001b[1;32m---> 32\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_mock_csc_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 初始化分词器\u001b[39;00m\n\u001b[0;32m     35\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-chinese\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m, in \u001b[0;36mcreate_mock_csc_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_mock_csc_dataset\u001b[39m():\n\u001b[0;32m     25\u001b[0m     ret \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_data_loader\u001b[49m:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x):\n\u001b[0;32m     28\u001b[0m             ret[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrected_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: y[i]})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, BertForMaskedLM, BertTokenizer, get_scheduler\n",
    "\n",
    "\n",
    "# 创建模拟的CSC数据集\n",
    "# def create_mock_csc_dataset():\n",
    "#     return {\n",
    "#         \"train\": [\n",
    "#             {\"original_text\": \"这是一个测试句子\", \"corrected_text\": \"这是一个测试句子\"},\n",
    "#             {\"original_text\": \"这是个测试句子\", \"corrected_text\": \"这是一个测试句子\"},\n",
    "#             {\"original_text\": \"这是一只猫\", \"corrected_text\": \"这是一只猫\"},\n",
    "#             {\"original_text\": \"我爱北京天安们\", \"corrected_text\": \"我爱北京天安门\"},\n",
    "#         ],\n",
    "#         \"validation\": [\n",
    "#             {\n",
    "#                 \"original_text\": \"这是一个简单的例子\",\n",
    "#                 \"corrected_text\": \"这是一个简单的例子\",\n",
    "#             },\n",
    "#             {\"original_text\": \"他去了北景公园\", \"corrected_text\": \"他去了北海公园\"},\n",
    "#         ],\n",
    "#     }\n",
    "def create_mock_csc_dataset():\n",
    "    ret = {\"train\": [], \"valid\": []}\n",
    "    for x, y in train_data_loader:\n",
    "        for i, v in enumerate(x):\n",
    "            ret[\"train\"].append({\"original_text\": v, \"corrected_text\": y[i]})\n",
    "            ret[\"valid\"].append({\"original_text\": v, \"corrected_text\": y[i]})\n",
    "\n",
    "\n",
    "dataset = create_mock_csc_dataset()\n",
    "\n",
    "# 初始化分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "# 定义tokenize函数\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"original_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"corrected_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )[\"input_ids\"]\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": labels.squeeze(),\n",
    "    }\n",
    "\n",
    "\n",
    "# 将数据集转换为Hugging Face的Dataset对象\n",
    "train_dataset = Dataset.from_list(dataset[\"train\"])\n",
    "val_dataset = Dataset.from_list(dataset[\"validation\"])\n",
    "\n",
    "# 对数据集进行tokenize处理\n",
    "train_tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "val_tokenized_datasets = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 设置格式为pytorch tensor\n",
    "train_tokenized_datasets.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "val_tokenized_datasets.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "# 创建DataLoader\n",
    "train_dataloader = DataLoader(train_tokenized_datasets, shuffle=True, batch_size=4)\n",
    "eval_dataloader = DataLoader(val_tokenized_datasets, batch_size=4)\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 检查设备类型\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 优化器设置\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 学习率调度器\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# 训练循环\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # 验证步骤可以在这里添加\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# 示例预测\n",
    "def predict(text):\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_token_id = logits.argmax(dim=-1)[0]\n",
    "    corrected_text = tokenizer.decode(predicted_token_id, skip_special_tokens=True)\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "# 测试预测\n",
    "test_texts = [\"我爱北京天安们\", \"这是一只猫\"]\n",
    "for text in test_texts:\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Corrected: {predict(text)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0888153a-ed10-415c-be4a-ebdb4e150943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc_env",
   "language": "python",
   "name": "csc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
