{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6508b6d6-a22b-4dba-ba9b-805cfa20a99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T12:58:00.020240Z",
     "iopub.status.busy": "2024-12-09T12:58:00.020240Z",
     "iopub.status.idle": "2024-12-09T12:58:03.744194Z",
     "shell.execute_reply": "2024-12-09T12:58:03.744194Z",
     "shell.execute_reply.started": "2024-12-09T12:58:00.020240Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Implemented by myself\n",
    "from config import *\n",
    "from data_processer import CSCDataset\n",
    "from models import CombineBertModel, DecoderBaseRNN, DecoderTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c9c49d-ca57-4b70-8333-624dcc395a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T12:58:05.024995Z",
     "iopub.status.busy": "2024-12-09T12:58:05.024995Z",
     "iopub.status.idle": "2024-12-09T12:58:05.030130Z",
     "shell.execute_reply": "2024-12-09T12:58:05.030130Z",
     "shell.execute_reply.started": "2024-12-09T12:58:05.024995Z"
    }
   },
   "outputs": [],
   "source": [
    "def cal_err(raw_sentence, pred_sentence, corr_sentence):\n",
    "    matrices = [\"over_corr\", \"total_err\", \"true_corr\"]\n",
    "    char_level = {key: 0 for key in matrices}\n",
    "    sent_level = {key: 0 for key in matrices}\n",
    "\n",
    "    f1 = f2 = 0\n",
    "    for i, c in enumerate(raw_sentence):\n",
    "        pc, cc = pred_sentence[i], corr_sentence[i]\n",
    "\n",
    "        if cc != c:\n",
    "            char_level[\"total_err\"] += 1\n",
    "            char_level[\"true_corr\"] += pc == cc\n",
    "            f1 = 1\n",
    "        elif pc != cc:\n",
    "            char_level[\"over_corr\"] += 1\n",
    "            f2 = 1\n",
    "\n",
    "    if f1:\n",
    "        sent_level[\"true_corr\"] += all(pred_sentence == corr_sentence)\n",
    "        sent_level[\"total_err\"] += f1\n",
    "    sent_level[\"over_corr\"] += f2\n",
    "\n",
    "    return char_level, sent_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1008e61e-5560-4614-a9ae-78a4ed7391e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T12:58:06.032939Z",
     "iopub.status.busy": "2024-12-09T12:58:06.032939Z",
     "iopub.status.idle": "2024-12-09T12:58:06.039632Z",
     "shell.execute_reply": "2024-12-09T12:58:06.039632Z",
     "shell.execute_reply.started": "2024-12-09T12:58:06.032939Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, tokenizer, test_data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    matrices = [\"over_corr\", \"total_err\", \"true_corr\"]\n",
    "    test_char_level = {key: 0 for key in matrices}\n",
    "    test_sent_level = {key: 0 for key in matrices}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device).type(torch.float)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, src_mask=attention_mask)\n",
    "            logits = outputs.permute(0, 2, 1)\n",
    "\n",
    "            loss = F.cross_entropy(logits, labels, ignore_index=tokenizer.pad_token_id)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            t = torch.argmax(outputs, dim=-1)\n",
    "            nt = t * attention_mask\n",
    "            pred = tokenizer.batch_decode(nt, skip_special_tokens=True)\n",
    "\n",
    "            for i in range(len(t)):\n",
    "                char_level, sent_level = cal_err(input_ids[i], nt[i], labels[i])\n",
    "                test_char_level = {\n",
    "                    key: test_char_level[key] + v for key, v in char_level.items()\n",
    "                }\n",
    "                test_sent_level = {\n",
    "                    key: test_sent_level[key] + v for key, v in sent_level.items()\n",
    "                }\n",
    "        print(total_loss / len(test_data_loader), test_char_level, test_sent_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d944d7-b98f-4818-bfc8-44182a89a6ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T12:58:06.635319Z",
     "iopub.status.busy": "2024-12-09T12:58:06.635319Z",
     "iopub.status.idle": "2024-12-09T12:58:08.932053Z",
     "shell.execute_reply": "2024-12-09T12:58:08.932053Z",
     "shell.execute_reply.started": "2024-12-09T12:58:06.635319Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c75e7320-399a-433c-9a54-fc87a9fe59b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T12:58:10.028354Z",
     "iopub.status.busy": "2024-12-09T12:58:10.028354Z",
     "iopub.status.idle": "2024-12-09T12:58:10.054195Z",
     "shell.execute_reply": "2024-12-09T12:58:10.053686Z",
     "shell.execute_reply.started": "2024-12-09T12:58:10.028354Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing sighan dataset: 2339it [00:00, 749635.29it/s]\n",
      "preprocessing sighan dataset: 100%|███████████████████████████████████████████| 2339/2339 [00:00<00:00, 1168331.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共2339句，共73264字，最长的句子有171字\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing sighan dataset: 3437it [00:00, 957607.47it/s]\n",
      "preprocessing sighan dataset: 100%|███████████████████████████████████████████| 3437/3437 [00:00<00:00, 1147299.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共3437句，共170330字，最长的句子有258字\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CSCDataset([SIGHAN_train_dir_err, SIGHAN_train_dir_corr], tokenizer)\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, num_workers=0, shuffle=True, batch_size=16\n",
    ")\n",
    "\n",
    "test_dataset = CSCDataset([SIGHAN_train_dir_err14, SIGHAN_train_dir_corr14], tokenizer)\n",
    "test_data_loader = DataLoader(test_dataset, num_workers=0, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699498a-826b-4c67-aab6-aa5aa22b082e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7ea434-79e9-4310-b572-8c5367b84212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T12:58:16.108130Z",
     "iopub.status.busy": "2024-12-09T12:58:16.107129Z",
     "iopub.status.idle": "2024-12-09T12:58:16.117236Z",
     "shell.execute_reply": "2024-12-09T12:58:16.116230Z",
     "shell.execute_reply.started": "2024-12-09T12:58:16.108130Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, tokenizer, train_data_loader, test_data_loader=None, has_softmax=False\n",
    "):\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(\n",
    "            enumerate(train_data_loader),\n",
    "            desc=f\"Epoch:{epoch+1}/{epochs}\",\n",
    "            total=len(train_data_loader),\n",
    "        )\n",
    "\n",
    "        for i, batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device).type(torch.float)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, src_mask=attention_mask)\n",
    "            logits = outputs.permute(0, 2, 1)  # (batch_size, vocab_size, seq_len)\n",
    "\n",
    "            # 反向传播在这，故labels不需要传入模型\n",
    "            loss = F.cross_entropy(logits, labels, ignore_index=tokenizer.pad_token_id)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": \"{:.3f}\".format(loss.item())})\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t = torch.argmax(outputs, dim=-1) if not has_softmax else outputs\n",
    "            nt = t * attention_mask\n",
    "            pred = tokenizer.batch_decode(nt, skip_special_tokens=True)\n",
    "        # print(pred)\n",
    "        # print(f\"origin{tokenizer.batch_decode(labels, skip_special_tokens=True)}\")\n",
    "\n",
    "        for i, v in enumerate(nt):\n",
    "            r, l = input_ids[i], labels[i]\n",
    "            print(tokenizer.decode(r, skip_special_tokens=True))\n",
    "            print(tokenizer.decode(v, skip_special_tokens=True))\n",
    "            print(tokenizer.decode(l, skip_special_tokens=True))\n",
    "            print(cal_err(r, v, l))\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_data_loader)}\")\n",
    "\n",
    "        # dev\n",
    "        if test_data_loader:\n",
    "            test(model, tokenizer, test_data_loader)\n",
    "\n",
    "        # 释放不必要的内存\n",
    "        # del input_ids, attention_mask, labels, outputs, loss\n",
    "        # torch.cuda.empty_cache()\n",
    "        # gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9c6a5-6c36-4ecb-8413-d6ca9ffec218",
   "metadata": {},
   "source": [
    "#### BERT + LSTM\n",
    "**长度不一致问题**：预测的时候有些字符预测为了 **[SEP]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c6352d-f799-4aad-89ac-614b579f3190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T11:01:29.099429Z",
     "iopub.status.busy": "2024-12-09T11:01:29.099429Z",
     "iopub.status.idle": "2024-12-09T11:02:20.675947Z",
     "shell.execute_reply": "2024-12-09T11:02:20.674941Z",
     "shell.execute_reply.started": "2024-12-09T11:01:29.099429Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1/5:  42%|████████████████████████▍                                 | 62/147 [00:48<01:06,  1.28it/s, loss=6.152]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     10\u001b[0m decoder_model \u001b[38;5;241m=\u001b[39m DecoderBaseRNN(\n\u001b[0;32m     11\u001b[0m     rnn\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLSTM,\n\u001b[0;32m     12\u001b[0m     input_size\u001b[38;5;241m=\u001b[39mencoder_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m     13\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[0;32m     14\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m CombineBertModel(encoder_model\u001b[38;5;241m=\u001b[39mencoder_model, decoder_model\u001b[38;5;241m=\u001b[39mdecoder_model)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, tokenizer, train_data_loader, test_data_loader)\u001b[0m\n\u001b[0;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 31\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)})\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m     34\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "encoder_model = BertModel.from_pretrained(checkpoint)\n",
    "\n",
    "# The Hyperparameters can be defined in config.py\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "\n",
    "decoder_model = DecoderBaseRNN(\n",
    "    model=nn.LSTM,\n",
    "    input_size=encoder_model.config.hidden_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "\n",
    "model = CombineBertModel(encoder_model=encoder_model, decoder_model=decoder_model)\n",
    "\n",
    "train(model, tokenizer, train_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b2bad-16b0-43d5-8a79-59563e6d0b31",
   "metadata": {},
   "source": [
    "#### BERT + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e3965-934c-4aea-91d6-81b4986b595e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-09T11:02:20.677018Z",
     "iopub.status.idle": "2024-12-09T11:02:20.677018Z",
     "shell.execute_reply": "2024-12-09T11:02:20.677018Z",
     "shell.execute_reply.started": "2024-12-09T11:02:20.677018Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model = BertModel.from_pretrained(checkpoint)\n",
    "\n",
    "# The Hyperparameters can be defined in config.py\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "\n",
    "decoder_model = DecoderBaseRNN(\n",
    "    model=nn.GRU,\n",
    "    input_size=encoder_model.config.hidden_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "\n",
    "model = CombineBertModel(encoder_model=encoder_model, decoder_model=decoder_model)\n",
    "\n",
    "train(model, tokenizer, train_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd0f40-000b-423e-9343-1fae4754abf4",
   "metadata": {},
   "source": [
    "#### BERT + transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635e5020-9a3f-4298-be03-ebb42c11e14f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T12:59:25.629997Z",
     "iopub.status.busy": "2024-12-09T12:59:25.628996Z",
     "iopub.status.idle": "2024-12-09T12:59:29.978898Z",
     "shell.execute_reply": "2024-12-09T12:59:29.978898Z",
     "shell.execute_reply.started": "2024-12-09T12:59:25.629997Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m      7\u001b[0m decoder_model \u001b[38;5;241m=\u001b[39m DecoderTransformer(\n\u001b[0;32m      8\u001b[0m     input_size\u001b[38;5;241m=\u001b[39mencoder_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m      9\u001b[0m     nhead\u001b[38;5;241m=\u001b[39mnhead,\n\u001b[0;32m     10\u001b[0m     num_encoder_layers\u001b[38;5;241m=\u001b[39mnum_encoder_layers,\n\u001b[0;32m     11\u001b[0m     num_decoder_layers\u001b[38;5;241m=\u001b[39mnum_decoder_layers,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m CombineBertModel(encoder_model\u001b[38;5;241m=\u001b[39mencoder_model, decoder_model\u001b[38;5;241m=\u001b[39mdecoder_model)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, tokenizer, train_data_loader, test_data_loader, has_softmax)\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda3\\envs\\csc_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "encoder_model = BertModel.from_pretrained(checkpoint)\n",
    "\n",
    "nhead = 2\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "\n",
    "decoder_model = DecoderTransformer(\n",
    "    input_size=encoder_model.config.hidden_size,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    ")\n",
    "\n",
    "model = CombineBertModel(encoder_model=encoder_model, decoder_model=decoder_model)\n",
    "\n",
    "train(model, tokenizer, train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c23acc-8c14-4b9d-b76e-9695adba7324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc_env",
   "language": "python",
   "name": "csc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
